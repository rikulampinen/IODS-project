# Chapter 3: Logistic regression

### Data wrangling and performing a logistic regression analysis

#### *Work of week 46 (11.11. - 17.11.2019)*

***
## 1. Data wrangling

R script is available on my github repository. To get to the script click [here](https://github.com/rikulampinen/IODS-project/blob/master/data/create_alc.R)

***

## 2. Analysis  

## 2.1. Read the prepared data set

The working directory is set using `setwd()` and the data file "alc" prepared in the data wrangling part is read using `read.table()`function. Afterwards the data frame is checked.

```{r, setwd, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
setwd("C:/Users/richla/OneDrive/1 C - R-Folder/11-IODS-course/IODS-project/")
getwd()
```

```{r, readdata, echo=TRUE, results='markdown', message=FALSE, warning=FALSE}
alc <- read.table(file = 
       "C:/Users/richla/OneDrive/1 C - R-Folder/11-IODS-course/IODS-project/data/alc_table.txt", stringsAsFactors = TRUE) 
       # read the data file and leave the binary values as they are

# Check the data frame
dim(alc)
str(alc)
colnames(alc)
``` 
  
The data frame consists of 35 variables with 382 observations.
  
## 2.2. The data set

The provided data is from a approach on student achievement in secondary education in two Portugal schools.  
It was collected with school reports and questionaires and it includes:  
  
* student grades
* demographic features
* social features
* school related features  
  
The data consists of two data sets regarding the students' performance in Mathematics (mat) and Portuguese language (por). In a publication by [Cortez & Silva, 2008](http://www3.dsi.uminho.pt/pcortez/student.pdf) this data was already used.  
  
In the data wrangling part of this exercise the two data sets of mat and por were merged into one data set named "alc".  Columns (variables) which were not used in merging the data set were combined by averaging.
Since we are interested to analyse the alcohol consumption of the students we created two new variables in the data set:  

* "alc_use"  --> calculated average of weekday ("Dalc") and weekend ("Walc") alcohol consumption  
* "high_use" --> logical values created as TRUE or FALSE depending if the "alc_use" of a student is higher than 2  

All other variables are explained in detail [here](https://archive.ics.uci.edu/ml/datasets/Student+Performance) (check attribute information).  
  
***
  
## 2.3. Performing the analyses

## 2.3.1. Choose variables for the analyses

The task of this exercise is to figure out the relationships between high and low alcohol consumption and other variables in this data set.
I have choosen to run the analyses on following four variables:  

* sex      --> student's sex (binary: 'F' - female or 'M' - male) 
* failures --> number of past class failures (numeric: n if 1<=n<3, else 4)
* famrel   --> quality of family relationships (numeric: from 1 - very bad to 5 - excellent) 
* absences --> number of school absences (numeric: from 0 to 93) 

I have choosen these four variables for the data analyses for following reasons:  

1. The student's gender ("sex") might have kind a great influence on the alcohol consumption. Young men tent to drink more alcohol for several reasons, women are able to control themselves (don't take my steriotypical expressions serious please!). 

2. The number of class failures ("failures") of a student. Someone who fails in school more often might have a higher alcohol consumption then others (personal opionion) both as a causation of higher use or as a response to failure.  

3. I think the  overall quality of family relationships ("famrel") might have a huge effect on  a students alcohol consumption. I know, this is a very, very conservative point of view but in a country like Portugal (catholic, maybe more patriachic family structures) that might have an influence in young peoples alcohol consumption.  

4. The number of school absences. I think if someone is absent from school they maybe spent their time also with drinking alcohol. I know there are other things to do as well, but when I was that age drinking beer was somehow a thing to spent your time.    

***
  
## 2.3.2. Explore the distributions of the chosen variables numerically and graphically

The task is to explore the distribution of the variables I have choosen and the relationship with alcohol consumption numerically and graphically.  

```{r, loadpackage, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
library(tidyr)
library(dplyr)
library(ggplot2)
library(knitr)
library(kableExtra)
```

First I check how many female and male students are there:  
  
  
```{r, exploredata1,  echo=FALSE, results='markdown', message=FALSE, warning=FALSE}
# check the number of female and male students
alc %>% group_by(sex) %>% summarise(count = n()) -> fm
knitr::kable(fm, caption="Students") %>% kable_styling(bootstrap_options = "hover", full_width = FALSE, position = "center")
```
***
  
### 2.3.2.1. General data overview  

```{r, exploredata2,  echo=FALSE, results='hide', message=FALSE, warning=FALSE}
# gather columns into key-value pairs and draw a bar plot of the variables
gather(alc) %>% ggplot(aes(x = value)) + facet_wrap("key", scales = "free") + geom_bar(fill = "darkgreen", alpha = 0.7) + ggtitle("Overview bar plot of gathered variables") + xlab("Value") + ylab("Count") -> alc_gather

ggsave("alc_gather.png", 
       plot = alc_gather, path = "C:/Users/richla/OneDrive/1 C - R-Folder/11-IODS-course/IODS-project/data/", scale = 2,
       dpi = 300)
``` 
![](C:/Users/richla/OneDrive/1 C - R-Folder/11-IODS-course/IODS-project/data/alc_gather.png)
  
  
This bar plot gives us an overview over the different variables and might give a hint which variables have a strong relationship with alcohol consumption.
  
***

### 2.3.2.2. General alcohol consumption overview  

```{r, exploredata3,  echo=FALSE, results='markdown', message=FALSE, warning=FALSE}
# alc_use general shown with gender
ggplot(data = alc, aes(x = alc_use, fill = sex)) + geom_bar() + xlab("Average alcohol consumption (1 = lowest / 5 = highest)") + ylab("Count") + ggtitle("General alcohol consumption") + scale_fill_discrete("Student's gender", labels = c("Female", "Male"))
```  
  
The graph shows the average alcohol consumption count of the students. 1 is the lowest consumption, 5 the highest consumption. The amount of students with a low consumption is around 130 of 385.  

*** 
  
### 2.3.2.3. High alcohol consumption of students by gender
  
```{r, exploredata4,  echo=FALSE, results='markdown', message=FALSE, warning=FALSE}
# high_use by gender
ggplot(data = alc, aes(x = alc_use, fill = high_use)) + facet_wrap("sex") + geom_bar() + xlab("Student's alcohol consumption") + ggtitle("Alcohol consumption by gender") + ylab("Count") + scale_fill_discrete("High alcohol use")
```
  
This graph gives an overview of the alcohol consumption of students devided by gender. With this graph you can see the amount of male or female students with higher alcohol consumption easier. High alcohol consumption (> 2) is indicated with turquise colour, lower alcohol consumption is indicated in red colour. You can see that the amount of male students with a high alcohol consumption is higher compared to female students.  

***

### 2.3.2.4. High alcohol consumption and student's class failures

```{r, exploredata5,  echo=FALSE, results='markdown', message=FALSE, warning=FALSE}
# past class failures
ggplot(data = alc, aes(x = high_use, y = failures, fill = sex)) + geom_violin()  + ggtitle("Student's class failures by high alcohol use and sex") + xlab("High alcohol consumption") + ylab("Class failures") + scale_fill_discrete("Student's gender", labels = c("Female", "Male"))
```
  
What is seen in this graph is that students with higher alcohol consumptuion tent to have a higher class failing rate. Female students with higher alcohol use have a higher count at 1 failure, male students have a higher count at 2 failures or 3 failures. 

Generally, you see that students which have no high alcohol consumption fail less in classes.

***

### 2.3.2.5. High alcohol consumption and student's family relationship status

```{r, exploredata6,  echo=FALSE, results='markdown', message=FALSE, warning=FALSE}
# students family relationship quality
ggplot(data = alc, aes(x = famrel,  fill = sex)) + geom_bar() + facet_wrap("high_use") + ggtitle("Student's family relationship quality by high alcohol consumption") + xlab("Quality of family relationships (1 = very bad / 5 = excellent)") + ylab("Count") + scale_fill_discrete("Student's gender", labels = c("Female", "Male"))
```
  
There are more students with a low alcohol conumption where the family relationship is good (4-5). Interestingly the amount of students with high alcohol consumption is also increasing with the quality of the family relationship status (higher count at status 3-5).
  
***
  
### 2.3.2.6. High alcohol consumption and student school abcences

```{r, exploredata7,  echo=FALSE, results='markdown', message=FALSE, warning=FALSE}
# boxplot of high_use and school absences
ggplot(alc, aes(x = high_use, y = absences, fill = sex)) + geom_violin() + ylab("School absences") + xlab("High alcohol consumption") + ggtitle("Student absences by alcohol consumption") + scale_fill_discrete("Student's gender", labels = c("Female", "Male"))
```

High alcohol use seems to have an influence on the rate of school absences. Students with a higher alcohol consumption have an increased absance amount. Females tent to be more absent than males, also if they don't fall in the group of high alcohol consumers.

***

## 2.3.3. Logistic regression - explore the relationship between the choosen variables and the high alcohol consumption

### 2.3.3.1. Calculation of the logistic regression model   
**I'm using in the model the four variables I have choosen: sex, failures, famrel and absences**

```{r, glm1,  echo=TRUE, results='hide', message=FALSE, warning=FALSE}
# cv_glm, choosen variables logistic model
cv_glm <- glm(high_use ~ sex + failures + famrel + absences, data = alc, family = "binomial")

# Summary of the model (cv_glm_sum)
cv_glm_sum <- summary(cv_glm)
```

```{r, output, echo=FALSE, results='markdown', message=FALSE, warning=FALSE}
# print out the coefficients of the model
cv_glm_sum

knitr::kable(cv_glm_sum$coefficients, digits = 3, align = "l", justify = "left", caption="Model coefficients of choosen variables") %>% kable_styling(bootstrap_options = "striped", full_width = F, position = "center")
```
    
The model output shows nice results and also significant one. So which variables influance high alcohol consumption?  

* Being a male student significantly (p = 5.37e-05 ***) increases the probability of high alcohol consumption by ~1.
* Class failure have a smaller probability but still have a positive effect on high consumption (p = 0.0243 *).
* The quality of family relationships has a negative effect on high alcohol consumption (p = 0.0314 *).
* School absences show also a significant positive effect on high alcohol consumption (5.83e-05 ***).

This is a first result. But we are not going to use these results as our final interpretation of the logistic regression. We are checking now the odds ratios of the model.

***
  
### 2.3.3.2. Calculating the odds ratios

```{r, odds, echo=FALSE, results='hide', message=FALSE, warning=FALSE}

# compute odds ratios of choosen variables (or_cv)
or_cv <- coef(cv_glm) %>% exp
or_cv 

# compute confidence intervals of the choosen variables (ci_cv)
ci_cv <- confint(cv_glm) %>% exp
ci_cv
```

```{r, output2, echo=FALSE, results='markdown', message=FALSE, warning=FALSE }
# print out the odds ratios with their confidence intervals
od_conf <- cbind(or_cv, ci_cv) 
colnames(od_conf) <- c("odds ratios","2.5 %", "97.5 %")
knitr::kable(od_conf, digits = 3, align = "l",  caption="Odds ratios and confidental intervals") %>% kable_styling(bootstrap_options = "striped", full_width = F, position = "center")
```
     
Interpretation of odds ratios:
If the odds ratio is lower than 1 the risk of high alcohol consumption is lower, if it is greater than 1 the risk of high consumption increases.
But we also have to have a look on the confidential interals - if the values cross the value of 1 the odds ratio cannot be significant!
The odds ratios of this logistic model lead to following interpretation.

* **sexM** has a value of 2.7, the confidence interval is between 1.6 and 4.4.  So this oods ratio tells us that the male students have a 2.7 times higher probability of consuming alcohol on a higher level.  
* **failures** has a value of 1.5, the confidence interval is between 1.1 and 2.2.  Failing in classes also increases the risk of high alcohol consumption by 1.5. So failing once increases the high alcohol consumption probability by 1.5. 
* **famrel** has a value of 0.8, the confidence interval is between 0.6 and 1.  It seems that the family relationships have a negative effect on high alcohol consumption.
* **absences** has a value of 1.095, the confidence interval is between 1.050 and 1.147. The amount of school absences increase the probability of higher alcohol consumption.  


***

### 2.3.3.3. Explore the predictive power of the model

I calculated the probability of high alcohol use and added the data to the "alc" data frame. Then the prediction of a high alcohol consumption is defined to be a probability greater than 0.5 and also added this outcome to the "alc" data frame.  
Here are the last 20 data points with the probability and prediction outputs:
```{r, prediction, echo=FALSE, results='markdown', message=FALSE, warning=FALSE}
# predict() the probability of high_use
probabilities <- predict(cv_glm, type = "response")

# add the predicted probabilities to 'alc'
alc <- mutate(alc, probability = probabilities)

# use the probabilities to make a prediction of high_use
alc <- mutate(alc, prediction = probability > 0.5)

# check the updated data frame (last ten observations)
select(alc, age, Pstatus, failures, famrel, high_use, probability, prediction) %>% tail(20)

# tabulate the target variable versus the predictions (pred2_2)
pred2_2 <- table(high_use = alc$high_use, prediction = alc$prediction)
```


```{r, crosstable, echo=FALSE, results='markdown', message=FALSE, warning=FALSE}
knitr::kable(pred2_2, align = "l",  caption="2 x 2 cross tabulation" ) %>% kable_styling(bootstrap_options = "striped", full_width = F, position = "center") %>% add_header_above(c(" " = 1, "Prediction" = 2)) %>%  pack_rows("High use", 1, 2)

```
 
Interpretation of the prediction:  
The 2 x 2 cross table gives us the results of the model prediction for high alcohol consumption compared to the real numbers.  
The model resulted in 253 correct false results and 82 false negative results. This means that the prediction fails to be correct. In 82 cases the model predicted that the student has no high alcohol consumption, but the student actually has high alcohol consumption. This false negative results should be much smaller.  
In 15 cases the model predicts high alcohol consumption but is wrong, that's a false positive prediction.
253 cases were correctly predicted as false and 32 as true for high alcohol consumption.  

***
  
### 2.3.3.4. Graphic visualization of actual values and predictions

```{r, prediction_graph, echo=FALSE, results='markdown', message=FALSE, warning=FALSE }
# initialize a plot of 'high_use' versus 'probability' in 'alc'
ggplot(alc, aes(x = probability, y = high_use, col = prediction)) + geom_point() + xlab("Probability value") + ylab("High alcohol use") + ggtitle("High alcohol use versus probability") + scale_color_discrete("Prediction")
```
  
The graph represent the 2x2 cross table in a graphical view with the false negative values on top left of the graph. 82 cases predicted at FALSE but with a TRUE high alcohol use.  
  

```{r, target_variable, echo=FALSE, results='markdown', message=FALSE, warning=FALSE }
# tabulate the target variable versus the predictions (tvp)
tvp <- table(high_use = alc$high_use, prediction = alc$prediction) %>% prop.table() %>% addmargins()

knitr::kable(tvp, digits = 4, align = "l",  caption="Target variable versus the predictions" ) %>% kable_styling(bootstrap_options = "striped", full_width = F, position = "center") %>% add_header_above(c(" " = 1, "Prediction" = 3)) %>%  pack_rows("high use", 1, 3)
```
  
Is the prediction of high alcohol use correct of not?  
This propability table gives us the correct predictions, false negative and false positive values as fractions of the total number. Results in fraction values between 0 and 1.  
So 21.5 % of are false negative predictions! 3.9 % are false positive predictions.

***

### 2.3.3.5. Computing the loss function - testing error

To measure the performance of of the logistic regression model we perform the loss function to calculate the average number of wrong predictions of our model.  

This results to following value:

```{r, loss_function, echo=FALSE, results='markdown', message=FALSE, warning=FALSE }
# define a loss function (mean prediction error)
loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}

# call loss_func to compute the average number of wrong predictions in the (training) data
loss_func(class = alc$high_use, prob = alc$probability)
```
  
So 25.4% of all predictions are wrong! Every fourth prediction is incorrect (to check that you can calculate the sum of false positive and false negative values from the Target variable versus the predictions table: 21.5 + 3.9 = 25.4).  
**So on the training data the model has a prediction error of 25.4 %.**

***

### 2.3.3.6. Cross validation - the training error

Here we are testing the how good our model is on unseen data. The loss function value is computed on a data which we did not use to to train the model. If the value is low it is good! 
We are performing the cross validation so that we test the logistic regression models on a defined set of observations. Here we choose the value of 10 - so the dataset will be split in 10 groups and 1 group of these 10 will be tested.

```{r, cross_validation1, echo=TRUE, results='markdown', message=FALSE, warning=FALSE }
# K-fold cross-validation
library(boot)
cross_val<- cv.glm(data = alc, cost = loss_func, glmfit = cv_glm, K = 10)
```

The cross validation results to following value:
```{r, cross_validation2, echo=TRUE, results='markdown', message=FALSE, warning=FALSE }
# average number of wrong predictions in the cross validation
cross_val$delta[1]
```

This is the average number of wrong predictions in the cross validation.
**The prediction error on the testing data is higher compared to the training data!**  

Be aware of it that this value is changing with each new computing because the function computes everytime another set of observations with the logistic regression model.  

The cross validation tells us if the model is too much in the data --> if the model is more generalized the value of cross validation shall be higher than the loss function value. 
  
With the cross validation you check your model with the same data - the outcome should be higher number than the loss function --> that shows that the model is not too much in the data --> so the model is more kind of a general one!

***

## Bonus tasks

### Bonus task 1

The 10-fold cross-validation of my model was computed in section 2.3.3.6. above. My logistic model does not have a better performance compared to the DataCamp model (both have an error value of ~ 0.26).  

I think a better model could be found - maybe more variables need to be used in the model, for example the age of the students or if students are in a relationship. These factors could make the model more accurate.  

***

### Bonus task 2

#### Cross-validations with different logistic regression models

```{r, model_table, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
model_graph <- data.frame("predictors" = c(8,7,6,5,4,3),
                     "training_error" = c(8,7,6,5,4,3),
                     "testing_error" = c(8,7,6,5,4,3))

model_graph
```

**Model 1: 8 variables (sex, age, Pstatus, guardian, failures, famrel, freetime, absences)**

```{r, cv_model8, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
#logistic regression model with 8 variables
mod8_glm <- glm(high_use ~ sex + age + Pstatus + guardian + failures + famrel + freetime + absences, 
                data = alc, family = "binomial")

# predict() the probability of high_use
probabilities8 <- predict(mod8_glm, type = "response")

# add the predicted probabilities to 'alc'
alc <- mutate(alc, probability8 = probabilities8)

#calculate training error
lf8 <- loss_func(class = alc$high_use, prob = alc$probability8)

#calculate the testing error
mod8_cross_val<- cv.glm(data = alc, cost = loss_func, glmfit = mod8_glm, K = 10)
```

**Model 2: 7 variables (sex, age, guardian, failures, famrel, freetime, absences) - Pstatus**

```{r, cv_model7, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
#logistic regression model with 7 variables
mod7_glm <- glm(high_use ~ sex + age + guardian + failures + famrel + freetime + absences, 
                data = alc, family = "binomial")

# predict() the probability of high_use
probabilities7 <- predict(mod7_glm, type = "response")

# add the predicted probabilities to 'alc'
alc <- mutate(alc, probability7 = probabilities7)

#calculate training error
lf7 <- loss_func(class = alc$high_use, prob = alc$probability7)

#calculate the testing error
mod7_cross_val<- cv.glm(data = alc, cost = loss_func, glmfit = mod7_glm, K = 10)
```

**Model 3: 6 variables (sex, age, failures, famrel, freetime, absences) - Pstatus, guardian**

```{r, cv_model6, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
#logistic regression model with 6 variables
mod6_glm <- glm(high_use ~ sex + age + failures + famrel + freetime + absences, 
                data = alc, family = "binomial")

# predict() the probability of high_use
probabilities6 <- predict(mod6_glm, type = "response")

# add the predicted probabilities to 'alc'
alc <- mutate(alc, probability6 = probabilities6)

#calculate training error
lf6 <- loss_func(class = alc$high_use, prob = alc$probability6)

#calculate the testing error
mod6_cross_val<- cv.glm(data = alc, cost = loss_func, glmfit = mod6_glm, K = 10)
```

**Model 4: 5 variables (sex, age, failures, freetime, absences) - Pstatus, guardian, famrel**

```{r, cv_model5, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
#logistic regression model with 5 variables
mod5_glm <- glm(high_use ~ sex + age + failures + freetime + absences, 
                data = alc, family = "binomial")

# predict() the probability of high_use
probabilities5 <- predict(mod5_glm, type = "response")

# add the predicted probabilities to 'alc'
alc <- mutate(alc, probability5 = probabilities5)

#calculate training error
lf5 <- loss_func(class = alc$high_use, prob = alc$probability5)

#calculate the testing error
mod5_cross_val<- cv.glm(data = alc, cost = loss_func, glmfit = mod5_glm, K = 10)
```

**Model 5: 4 variables (sex, age, failures, absences) - Pstatus, guardian, famrel, freetime**

```{r, cv_model4, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
#logistic regression model with 5 variables
mod4_glm <- glm(high_use ~ sex + age + failures + absences, 
                data = alc, family = "binomial")

# predict() the probability of high_use
probabilities4 <- predict(mod4_glm, type = "response")

# add the predicted probabilities to 'alc'
alc <- mutate(alc, probability4 = probabilities4)


#calculate training error
lf4 <- loss_func(class = alc$high_use, prob = alc$probability4)

#calculate the testing error
mod4_cross_val<- cv.glm(data = alc, cost = loss_func, glmfit = mod4_glm, K = 10)
```

**Model 6: 3 variables (sex, failures, absences) - Pstatus, guardian, famrel, freetime, age**

```{r, cv_model3, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
#logistic regression model with 5 variables
mod3_glm <- glm(high_use ~ sex + failures + absences, 
                data = alc, family = "binomial")

# predict() the probability of high_use
probabilities3 <- predict(mod4_glm, type = "response")

# add the predicted probabilities to 'alc'
alc <- mutate(alc, probability3 = probabilities3)

#calculate training error
lf3 <- loss_func(class = alc$high_use, prob = alc$probability3)

#calculate the testing error
mod3_cross_val<- cv.glm(data = alc, cost = loss_func, glmfit = mod3_glm, K = 10)
```
  

```{r, preparegraph, echo=FALSE, results='markdown', message=FALSE, warning=FALSE}
# add the values to the model_graph data frame
model_graph$training_error[1] <- lf8
model_graph$testing_error[1] <-mod8_cross_val$delta[1]

model_graph$training_error[2] <- lf7
model_graph$testing_error[2] <-mod7_cross_val$delta[1]

model_graph$training_error[3] <- lf6
model_graph$testing_error[3] <-mod6_cross_val$delta[1]

model_graph$training_error[4] <- lf5
model_graph$testing_error[4] <-mod5_cross_val$delta[1]

model_graph$training_error[5] <- lf4
model_graph$testing_error[5] <-mod4_cross_val$delta[1]

model_graph$training_error[6] <- lf3
model_graph$testing_error[6] <-mod3_cross_val$delta[1]

ggplot() + 
  geom_line(aes(x = model_graph$predictors, y = model_graph$training_error), colour = "red", size = 1) + 
  geom_line(aes(x = model_graph$predictors, y = model_graph$testing_error), colour = "green", size = 1 ) +
  ylab("Training & testing error") +
  xlab("Number of logistic model variables") + 
  ggtitle("Training and testing error values")
```       
  
The graph shows a red line representing the training error values and a green line representing the testing error values. You can see how the error values are changing. As more variables are used for the logistic regression model as higher the training and testing error becomes. It seems the model is then too "deep" in the data.

***